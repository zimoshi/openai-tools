import openai, sys

client = openai.OpenAI()

def voice_to_text(audio_file_path, model="whisper-1"):
    """
    The function `voice_to_text` takes an audio file path as input, converts the audio to text using a specified model, and returns the transcribed text.
    
    :param audio_file_path: The `audio_file_path` parameter is a string that represents the file path to the audio file that you want to convert to text. This function `voice_to_text` takes this file path as input, reads the audio file, and then uses a client to transcribe the audio into text using a
    :return: The function `voice_to_text` returns the transcribed text from the audio file using the specified model.
    """
    with open(audio_file_path, "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model=model,
            file=audio_file
        )
    return transcript.text

def chat_response(prompt, systemprompt="You are a helpful assistant.", model="gpt-4") -> str:
    """
    The `chat_response` function generates a response using a specified model based on a prompt and a system prompt.
    
    :param prompt: The `prompt` parameter is the input text or message provided by the user that the chatbot will respond to. It is the message or query that the user wants a response to
    :param systemprompt: The `systemprompt` parameter is the message that the system, or the coding assistant, will respond with before the user prompt in the chat conversation. It is a default message that can be customized to set the tone or provide context for the conversation, defaults to You are a helpful assistant. (optional)
    :param model: The `model` parameter in the `chat_response` function specifies the language model that will be used to generate responses. In this case, the default model is set to "gpt-4". This model will be used by the function to generate responses based on the input prompt provided by the user, defaults to gpt-4 (optional)
    :return: The `chat_response` function takes a prompt from the user and a system prompt (defaulted to "You are a helpful assistant.") as input, then uses a specified model (defaulted to "gpt-4") to generate a response. The function creates a response using the provided prompts and returns the generated text response.
    """
    response = client.responses.create(
        model=model,
        messages=[
            {"role": "system", "content": systemprompt},
            {"role": "user", "content": prompt}
        ]
    )
    return response.text

def chat_completion(prompt: str, systemprompt: str = "You are a helpful assistant.", model: str = "gpt-4-turbo", file_ids: list[str] = None) -> str:
    """
    The function `chat_completion` takes a user prompt, a system prompt, and optional parameters to generate a chat response using a specified model.
    
    :param prompt: The `prompt` parameter in the `chat_completion` function is a string that represents the user's input or message that they want a response to
    :type prompt: str
    :param systemprompt: The `systemprompt` parameter in the `chat_completion` function is a string that represents the message or prompt that the system (in this case, the coding assistant) will provide in the conversation. It defaults to "You are a helpful assistant." if not specified when calling the function, defaults to You are a helpful assistant.
    :type systemprompt: str (optional)
    :param model: The `model` parameter in the `chat_completion` function specifies the language model that will be used for generating responses to the chat prompts. In this case, the default model is set to "gpt-4-turbo", which is a specific version of the GPT-4 model optimized, defaults to gpt-4-turbo
    :type model: str (optional)
    :param file_ids: The `file_ids` parameter in the `chat_completion` function is a list of file IDs that can be used to include additional files or resources in the chat completion request. These files could contain additional context, information, or resources that the model can use to generate a more accurate response to the user
    :type file_ids: list[str]
    :return: The function `chat_completion` takes a user prompt, a system prompt, a model, and optional file IDs as input. It uses the OpenAI API to generate a response based on the prompts provided. The response is the completion of the user's prompt generated by the model specified."""
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": systemprompt},
            {"role": "user", "content": prompt}
        ],
        file_ids=file_ids if file_ids else None
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    if sys.argv[1] == "--audio-to-text":
        with open(sys.argv[2], "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file
            )
        print(transcript.text)
    elif sys.argv[1] == "--chat-response":
        response = client.responses.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": sys.argv[2]}
            ]
        )
        print(response.text)
    elif sys.argv[1] == "--chat-completion":
        prompt = sys.argv[2]
        file_ids = []

        if len(sys.argv) > 3:
            for path in sys.argv[3:]:
                uploaded = client.files.create(file=open(path, "rb"), purpose="assistants")
                file_ids.append(uploaded.id)

        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            file_ids=file_ids if file_ids else None
        )
        print(response.choices[0].message.content)
